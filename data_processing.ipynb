{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3d5f747-f6af-40e6-b029-b493cbef6e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "import texthero as hero\n",
    "import nltk\n",
    "from nltk import bigrams\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import WordCloud\n",
    "import itertools\n",
    "import networkx as nx\n",
    "from networkx.readwrite import json_graph\n",
    "import pylab\n",
    "import matplotlib.pyplot as plt\n",
    "from pyvis.network import Network\n",
    "import plotly.express as px\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1845d51e-df81-4eb0-b0cd-78b057a9d2d8",
   "metadata": {},
   "source": [
    "*Ce notebook suit le plan du notebook quarto final en donnant les codes des fonctions et opérations réalsiées pour chaque étape*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307ee3b3-769a-457d-b16c-009f96f92f90",
   "metadata": {},
   "source": [
    "# Constitution du corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11afcce-c5c8-45c6-8c72-3b8bf3fb28d9",
   "metadata": {},
   "source": [
    "## Scopus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ded41a8e-dfb5-4bae-b682-94b4b15b2795",
   "metadata": {},
   "outputs": [],
   "source": [
    "scopus_data = pd.read_csv(\"data/0_exported_raw_data/scopus-subset-2023-05-26.csv\", sep=\",\", encoding=\"utf-8\")\n",
    "# Step : only publis with DOi (otherwise too much duplicates)\n",
    "scopus_data = scopus_data[scopus_data.DOI.notna()].drop_duplicates(subset=['DOI'])\n",
    "# Step : remove empty title\n",
    "scopsu_data = scopus_data.loc[~(scopus_data['Title'].isna())]\n",
    "# Step: Drop columns\n",
    "scopus_data = scopus_data.drop(columns=['Author full names', 'Author(s) ID', 'Link'])\n",
    "# Step: Rename multiple columns\n",
    "scopus_data = scopus_data.rename(columns={'Year': 'publicationYear', 'Source title': 'sourceTitle', 'Document Type': 'documentType'})\n",
    "# Step: Rearranged the order of the columns\n",
    "scopus_data = scopus_data[['Source', 'DOI', 'Title', 'publicationYear', 'documentType', 'Authors', 'Publisher', 'sourceTitle', 'Abstract'] + []]\n",
    "# Step : save\n",
    "scopus_data.to_csv(\"data/1_primary_corpus/scopus_data.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe5de61-f1ac-421c-8e4b-4048ad5109c4",
   "metadata": {},
   "source": [
    "## Istex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "427f5b06-aaf5-4174-b92c-05f72cfeb969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting with the downloaded folder of json files from API\n",
    "def get_all_json_files(directory):\n",
    "    json_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".json\"):\n",
    "                json_files.append(os.path.join(root, file))\n",
    "    return json_files\n",
    "\n",
    "# Provide the root directory containing the subfolders and JSON files\n",
    "root_directory = \"data/0_exported_raw_data/istex-subset-2023-05-30\"\n",
    "all_json_files = get_all_json_files(root_directory)\n",
    "\n",
    "# Process data with keeping only documents with DOI\n",
    "istex_data = []\n",
    "for json_file in all_json_files:\n",
    "    if not 'manifest' in json_file:\n",
    "        with open(json_file, 'r', encoding='utf-8') as file:\n",
    "            result = {}\n",
    "            json_content = json.load(file)\n",
    "            #data = pd.json_normalize(json_content, max_level=2)\n",
    "            result[\"Source\"] = \"Istex\"\n",
    "            if 'doi' in json_content:\n",
    "                result[\"DOI\"] = json_content[\"doi\"][0]\n",
    "                result[\"Title\"] = json_content[\"title\"]\n",
    "                result[\"publicationYear\"] = json_content[\"publicationDate\"]\n",
    "                result[\"documentType\"] = json_content[\"genre\"][0]\n",
    "                if \"author\" in json_content:\n",
    "                    result[\"Authors\"] = \"; \".join([a[\"name\"] for a in json_content[\"author\"]])\n",
    "                if \"title\" in json_content[\"host\"]:\n",
    "                    result[\"sourceTitle\"] = json_content[\"host\"][\"title\"] \n",
    "                result[\"Publisher\"] = json_content[\"corpusName\"]\n",
    "                if \"abstract\" in json_content:\n",
    "                    result[\"Abstract\"] = json_content[\"abstract\"]\n",
    "            data = pd.DataFrame([result])\n",
    "            istex_data.append(data)\n",
    "istex_data = pd.concat(istex_data)\n",
    "# Step : deduplicate only applied on non empty DOI rows\n",
    "istex_data = istex_data[istex_data.DOI.notna()].drop_duplicates(subset=['DOI'])\n",
    "# Save \n",
    "istex_data.to_csv(\"data/1_primary_corpus/istex_data.csv\", index=False, encoding=\"utf-8\")\n",
    "# write DataFrame to an excel sheet\n",
    "#istex_data.to_excel('istex_data.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a86066-1575-4045-9542-ad51fb51f192",
   "metadata": {},
   "source": [
    "## Dédoublonnage et corpus final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4ad6ce61-cf4d-4863-96f8-04c41aeed01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = pd.concat([scopus_data,istex_data])\n",
    "for column_name in df_tmp.columns:\n",
    "    df_tmp[column_name] = df_tmp[column_name].astype('string')\n",
    "df_final = (df_tmp\n",
    "            .drop_duplicates(subset=['DOI'])\n",
    "            .reset_index(drop=True)\n",
    "           )\n",
    "# Step: Manipulate strings of 'documentType' via Find 'article' and Replace with 'Article'\n",
    "df_final[\"documentType\"] = df_final[\"documentType\"].str.replace('article', 'Article', regex=False)\n",
    "# Step: Filter on main documentType\n",
    "#df_final = df_final.loc[df_final['documentType'].isin(['Article', 'Review', 'Data paper', 'Book chapter', 'Book', 'Conference paper'])]\n",
    "df_final.to_csv(\"data/1_primary_corpus/extracted_corpus.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45279ce9-07c7-420c-942d-038447a7b26b",
   "metadata": {},
   "source": [
    "# Analyse lexicométrique sur les titres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de51419b-2980-4c80-adce-bd0c77ab18f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## divide strings into lists of substrings\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "def tokenize(x):\n",
    "    return [token for token in tokenizer.tokenize(x.strip()) if ((token != u\"\") & (len(token)>2))]\n",
    "\n",
    "## Lemmatization is the process of grouping together the different inflected forms of a word so they can be analyzed as a single item. Lemmatization is similar to stemming but it brings context to the words. So it links words with similar meanings to one word. \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(x):\n",
    "    return [lemmatizer.lemmatize(word) for word in x]\n",
    "\n",
    "## list ow words to string\n",
    "def list_to_string(x):\n",
    "    return ' '.join([word for word in x])\n",
    "\n",
    "\n",
    "def clean_text(df,col):\n",
    "    df[f\"{col}_clean_tmp\"] = df[f\"{col}\"].pipe(hero.clean)\n",
    "    df[f'{col}_token'] = df[f\"{col}_clean_tmp\"].apply(lambda x: tokenize(x))\n",
    "    df[f'{col}_token_list'] = df[f'{col}_token'].apply(lambda x: lemmatize(x))\n",
    "    df[f'{col}_cleaned'] = df[f'{col}_token_list'].apply(lambda x: list_to_string(x))\n",
    "    return (df\n",
    "            .drop(columns=[f\"{col}_clean_tmp\",f\"{col}_token\"])\n",
    "           )\n",
    "\n",
    "## Count vectorizer -> unigrams freq\n",
    "### on crée l'objet CountVectorizer\n",
    "count = CountVectorizer()\n",
    "## méthode fit_transform pour générer la matrice\n",
    "def bag_of_word_to_freq(df,col):\n",
    "    df = df[df[col].notna()]\n",
    "    bag_of_words = count.fit_transform(df[col])\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in count.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq\n",
    "\n",
    "## bigrams dataframe\n",
    "def get_bigrams(df,col):\n",
    "    list_tokens = df[col].to_list()\n",
    "    data = list(itertools.chain.from_iterable(list_tokens))\n",
    "    bi_grams = list(bigrams(data))\n",
    "    bigram_freq = nltk.FreqDist(bi_grams).most_common(len(bi_grams))\n",
    "    bigram_df = pd.DataFrame(bigram_freq,columns=['bigram', 'count'])\n",
    "    return bigram_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7093a0a1-a46e-4301-aaf7-9edb3b2ecbef",
   "metadata": {},
   "source": [
    "## Traitement NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "384961d6-6a10-4a9b-98eb-ef0e0ea305e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.read_csv(\"data/1_primary_corpus/extracted_corpus.csv\", sep=\",\", encoding=\"utf-8\")\n",
    "for c in df_final.columns:\n",
    "    df_final[c] = df_final[c].astype(str)\n",
    "df_final[\"Abstract\"] = df_final[\"Abstract\"].apply(lambda x: (x\n",
    "                                                 .replace(\"Abstract: \",\"\")\n",
    "                                                 .replace(\"ABSTRACT: \",\"\")\n",
    "                                                 .split(\"©\")[0])\n",
    "                                     )\n",
    "df_final = clean_text(df_final, \"Abstract\")\n",
    "df_final = clean_text(df_final, \"Title\")\n",
    "df_final.to_csv(\"data/2_results/nlp_corpus.csv\", index=False, encoding=\"utf-8\")\n",
    "df_final.to_excel('data/2_results/nlp_corpus.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80af8b40-b993-4fec-a808-19bd2f8bb164",
   "metadata": {},
   "source": [
    "## n-grammes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20fff1a9-e061-418e-84a7-11ba4be76787",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/2_results/nlp_corpus.csv\", sep=\",\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0dba252-7ea6-47fb-b289-dcd224dac643",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = {}\n",
    "for x in [\"Title\", \"Abstract\"]:\n",
    "    # si df = pd.read_csv(..) car df.to_csv(...) transforme les list en string\n",
    "    #df[f\"{x}_token_list\"] = df[f\"{x}_token_list\"].apply(literal_eval)\n",
    "    # Unigrammes\n",
    "    meta_df[f\"df_{x}_unigrams\"] = pd.DataFrame(bag_of_word_to_freq(df,f'{x}_cleaned'), columns = ['word' , 'count'])\n",
    "    # Bigrammes\n",
    "    meta_df[f\"df_{x}_bigrams\"] = get_bigrams(df,f'{x}_token_list')\n",
    "    meta_df[f\"df_{x}_bigrams\"]['bigram_to_string'] = meta_df[f\"df_{x}_bigrams\"]['bigram'].apply(lambda y: ','.join(y))\n",
    "    # Step : save\n",
    "    meta_df[f\"df_{x}_unigrams\"].to_csv(f\"data/2_results/{x}_unigrams.csv\", index=False, encoding=\"utf-8\")\n",
    "    meta_df[f\"df_{x}_unigrams\"].to_excel(f'data/2_results/{x}_unigrams.xlsx')\n",
    "    meta_df[f\"df_{x}_bigrams\"].to_csv(f\"data/2_results/{x}_bigrams.csv\", index=False, encoding=\"utf-8\")\n",
    "    meta_df[f\"df_{x}_bigrams\"].to_excel(f'data/2_results/{x}_bigrams.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
